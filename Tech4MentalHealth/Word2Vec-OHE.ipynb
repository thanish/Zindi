{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "import re \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec as w2v\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\thanisb\\\\Documents\\\\Competition\\\\Zindi\\\\Tech4MentalHealth\\\\Notebook'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DF.to_csv('../data/train_corrected.csv', index = False)\n",
    "test_DF.to_csv('../data/test_corrected.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the label to OHE\n",
    "train_DF = pd.concat([train_DF[['ID', 'text']], pd.get_dummies(train_DF.label)], axis = 1)\n",
    "labels = ['Alcohol', 'Depression', 'Drugs', 'Suicide']\n",
    "train_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Drugs</th>\n",
       "      <th>Suicide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUAVK39Z</td>\n",
       "      <td>i feel that it was better i dream happy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9JDAGUV3</td>\n",
       "      <td>why do i get hallucinations</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>419WR1LQ</td>\n",
       "      <td>i am stressed due to lack of financial support...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6UY7DX6Q</td>\n",
       "      <td>why is life important</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FYC0FTFB</td>\n",
       "      <td>how could i be helped to go through the depres...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID                                               text  Alcohol  \\\n",
       "0  SUAVK39Z            i feel that it was better i dream happy        0   \n",
       "1  9JDAGUV3                        why do i get hallucinations        0   \n",
       "2  419WR1LQ  i am stressed due to lack of financial support...        0   \n",
       "3  6UY7DX6Q                              why is life important        0   \n",
       "4  FYC0FTFB  how could i be helped to go through the depres...        0   \n",
       "\n",
       "   Depression  Drugs  Suicide  \n",
       "0           1      0        0  \n",
       "1           0      1        0  \n",
       "2           1      0        0  \n",
       "3           0      0        1  \n",
       "4           1      0        0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, remove_stopwords = True):\n",
    "    try:\n",
    "        #print(text)\n",
    "        processed_text = text.lower()\n",
    "        processed_text = re.sub(\"[^a-zA-Z]\",\" \",processed_text)\n",
    "        processed_text = processed_text.lower().split()\n",
    "        #print(\"processed\", processed_text)\n",
    "        words = processed_text\n",
    "        if remove_stopwords:\n",
    "                stops = set(stopwords.words(\"english\"))     \n",
    "                words = [w for w in processed_text if not w in stops]\n",
    "    except AttributeError:  # handling the case where the token is empty\n",
    "        words = ''\n",
    "    \n",
    "    return words\n",
    "\n",
    "def review_sentences(review, tokenizer, remove_stopwords=True):\n",
    "    # 1. Using nltk tokenizer\n",
    "    try:\n",
    "        raw_sentences = tokenizer.tokenize(review.strip())\n",
    "        \n",
    "    except AttributeError:  # handling the case where the token is empty\n",
    "        raw_sentences = ''\n",
    "\n",
    "    if len(raw_sentences) > 1 : raw_sentences = [\" \".join(raw_sentences)]\n",
    "\n",
    "    sentences = []\n",
    "    # 2. Loop for each sentence\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)>0:\n",
    "            sentences.append(preprocessing(raw_sentence, remove_stopwords))\n",
    "\n",
    "    # This returns the list of lists\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a Word2Vec model\n",
    "def w2vec_model(text, feature_embed):\n",
    "    num_features = feature_embed  # Word vector dimensionality\n",
    "    min_word_count = 1  # Minimum word count\n",
    "    num_workers = 4     # Number of parallel threads\n",
    "    context = 10        # Context window size\n",
    "    downsampling = 1e-3 # (0.001) Downsample setting for frequent words\n",
    "\n",
    "    print(\"Training model....\")\n",
    "    model = w2v.Word2Vec(text,\n",
    "                         workers=num_workers,\n",
    "                         size=num_features,\n",
    "                         min_count=min_word_count,\n",
    "                         window=context,\n",
    "                         sample=downsampling\n",
    "    )\n",
    "\n",
    "    #model.build_vocab(sentence)\n",
    "    model.train(text, total_examples= model.corpus_count, epochs=300)\n",
    "\n",
    "    # # To make the model memory efficient\n",
    "    # model.init_sims(replace=True)\n",
    "\n",
    "    # # Saving the model for later use. Can be loaded using Word2Vec.load()\n",
    "    # model_name = \"300features_40minwords_10context\"\n",
    "    # model.save(model_name)\n",
    "\n",
    "    print(\"Vocabulary shape\", model.wv.syn0.shape)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        #if counter%1000 == 0:\n",
    "        #    print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616\n",
      "309\n",
      "925\n",
      "Training model....\n",
      "Vocabulary shape (999, 50)\n",
      "(616, 50)\n",
      "(309, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thanisb\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "C:\\Users\\thanisb\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "Feature_dimension = 50\n",
    "\n",
    "train_content_sentence = []\n",
    "# TOkenizing all the text to\n",
    "for i, sent in enumerate(train_DF.text):\n",
    "    content = review_sentences(sent, tokenizer, remove_stopwords=False)\n",
    "    train_content_sentence += content\n",
    "print(len(train_content_sentence))\n",
    "\n",
    "test_content_sentence = []\n",
    "for i, sent in enumerate(test_DF.text):\n",
    "    content = review_sentences(sent, tokenizer, remove_stopwords=False)\n",
    "    test_content_sentence += content\n",
    "print(len(test_content_sentence))\n",
    "    \n",
    "Overall_content_sentence = train_content_sentence + test_content_sentence\n",
    "print(len(Overall_content_sentence))\n",
    "\n",
    "\n",
    "model = w2vec_model(Overall_content_sentence, feature_embed = Feature_dimension)\n",
    "\n",
    "train_content_embed = getAvgFeatureVecs(train_content_sentence, model, num_features = Feature_dimension)\n",
    "print(train_content_embed.shape)\n",
    "\n",
    "test_content_embed = getAvgFeatureVecs(test_content_sentence, model, num_features = Feature_dimension)\n",
    "print(test_content_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content Embed shape (616, 55)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embed_0</th>\n",
       "      <th>embed_1</th>\n",
       "      <th>embed_2</th>\n",
       "      <th>embed_3</th>\n",
       "      <th>embed_4</th>\n",
       "      <th>embed_5</th>\n",
       "      <th>embed_6</th>\n",
       "      <th>embed_7</th>\n",
       "      <th>embed_8</th>\n",
       "      <th>embed_9</th>\n",
       "      <th>...</th>\n",
       "      <th>embed_45</th>\n",
       "      <th>embed_46</th>\n",
       "      <th>embed_47</th>\n",
       "      <th>embed_48</th>\n",
       "      <th>embed_49</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Drugs</th>\n",
       "      <th>Suicide</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.230466</td>\n",
       "      <td>-0.182905</td>\n",
       "      <td>-0.332333</td>\n",
       "      <td>0.555513</td>\n",
       "      <td>0.333438</td>\n",
       "      <td>0.052286</td>\n",
       "      <td>0.029042</td>\n",
       "      <td>0.208676</td>\n",
       "      <td>-0.017010</td>\n",
       "      <td>0.382459</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.252820</td>\n",
       "      <td>-0.373233</td>\n",
       "      <td>0.247550</td>\n",
       "      <td>0.719784</td>\n",
       "      <td>0.329593</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SUAVK39Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.541881</td>\n",
       "      <td>-0.306691</td>\n",
       "      <td>0.797221</td>\n",
       "      <td>-0.078301</td>\n",
       "      <td>-0.510619</td>\n",
       "      <td>1.026608</td>\n",
       "      <td>0.078903</td>\n",
       "      <td>0.005831</td>\n",
       "      <td>-0.627429</td>\n",
       "      <td>0.446837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627112</td>\n",
       "      <td>0.852660</td>\n",
       "      <td>0.004821</td>\n",
       "      <td>-0.046888</td>\n",
       "      <td>-0.491767</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9JDAGUV3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.102223</td>\n",
       "      <td>0.483680</td>\n",
       "      <td>0.690245</td>\n",
       "      <td>0.056534</td>\n",
       "      <td>-0.078287</td>\n",
       "      <td>-0.295374</td>\n",
       "      <td>-0.022944</td>\n",
       "      <td>0.372534</td>\n",
       "      <td>-0.673362</td>\n",
       "      <td>-1.448769</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.905539</td>\n",
       "      <td>-0.457861</td>\n",
       "      <td>0.053081</td>\n",
       "      <td>-0.063998</td>\n",
       "      <td>0.319653</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>419WR1LQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.952695</td>\n",
       "      <td>-0.174957</td>\n",
       "      <td>0.483172</td>\n",
       "      <td>-0.312235</td>\n",
       "      <td>0.086527</td>\n",
       "      <td>1.537884</td>\n",
       "      <td>-0.072399</td>\n",
       "      <td>-0.711138</td>\n",
       "      <td>0.101181</td>\n",
       "      <td>0.946507</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.221083</td>\n",
       "      <td>1.092642</td>\n",
       "      <td>0.459013</td>\n",
       "      <td>0.397478</td>\n",
       "      <td>-0.567559</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6UY7DX6Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.294667</td>\n",
       "      <td>-0.573680</td>\n",
       "      <td>0.111463</td>\n",
       "      <td>0.267076</td>\n",
       "      <td>0.362375</td>\n",
       "      <td>0.494306</td>\n",
       "      <td>0.667499</td>\n",
       "      <td>-0.485858</td>\n",
       "      <td>-0.556440</td>\n",
       "      <td>0.203748</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530926</td>\n",
       "      <td>-0.875879</td>\n",
       "      <td>-0.089007</td>\n",
       "      <td>0.131266</td>\n",
       "      <td>0.226533</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>FYC0FTFB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    embed_0   embed_1   embed_2   embed_3   embed_4   embed_5   embed_6  \\\n",
       "0  0.230466 -0.182905 -0.332333  0.555513  0.333438  0.052286  0.029042   \n",
       "1  0.541881 -0.306691  0.797221 -0.078301 -0.510619  1.026608  0.078903   \n",
       "2 -0.102223  0.483680  0.690245  0.056534 -0.078287 -0.295374 -0.022944   \n",
       "3  0.952695 -0.174957  0.483172 -0.312235  0.086527  1.537884 -0.072399   \n",
       "4 -0.294667 -0.573680  0.111463  0.267076  0.362375  0.494306  0.667499   \n",
       "\n",
       "    embed_7   embed_8   embed_9  ...  embed_45  embed_46  embed_47  embed_48  \\\n",
       "0  0.208676 -0.017010  0.382459  ... -0.252820 -0.373233  0.247550  0.719784   \n",
       "1  0.005831 -0.627429  0.446837  ...  0.627112  0.852660  0.004821 -0.046888   \n",
       "2  0.372534 -0.673362 -1.448769  ... -0.905539 -0.457861  0.053081 -0.063998   \n",
       "3 -0.711138  0.101181  0.946507  ... -0.221083  1.092642  0.459013  0.397478   \n",
       "4 -0.485858 -0.556440  0.203748  ...  0.530926 -0.875879 -0.089007  0.131266   \n",
       "\n",
       "   embed_49  Alcohol  Depression  Drugs  Suicide        ID  \n",
       "0  0.329593        0           1      0        0  SUAVK39Z  \n",
       "1 -0.491767        0           0      1        0  9JDAGUV3  \n",
       "2  0.319653        0           1      0        0  419WR1LQ  \n",
       "3 -0.567559        0           0      0        1  6UY7DX6Q  \n",
       "4  0.226533        0           1      0        0  FYC0FTFB  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_content_embed_DF = pd.DataFrame(train_content_embed)\n",
    "train_content_embed_DF.columns = ['embed' + \"_\" + str(x) for x in range(0, Feature_dimension)]\n",
    "train_content_embed_DF[labels] = train_DF[labels]\n",
    "train_content_embed_DF['ID'] = train_DF.ID\n",
    "print(\"Content Embed shape\",train_content_embed_DF.shape)\n",
    "\n",
    "train_content_embed_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content Embed shape (309, 51)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embed_0</th>\n",
       "      <th>embed_1</th>\n",
       "      <th>embed_2</th>\n",
       "      <th>embed_3</th>\n",
       "      <th>embed_4</th>\n",
       "      <th>embed_5</th>\n",
       "      <th>embed_6</th>\n",
       "      <th>embed_7</th>\n",
       "      <th>embed_8</th>\n",
       "      <th>embed_9</th>\n",
       "      <th>...</th>\n",
       "      <th>embed_41</th>\n",
       "      <th>embed_42</th>\n",
       "      <th>embed_43</th>\n",
       "      <th>embed_44</th>\n",
       "      <th>embed_45</th>\n",
       "      <th>embed_46</th>\n",
       "      <th>embed_47</th>\n",
       "      <th>embed_48</th>\n",
       "      <th>embed_49</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.503976</td>\n",
       "      <td>-0.529186</td>\n",
       "      <td>0.269241</td>\n",
       "      <td>0.978526</td>\n",
       "      <td>-0.103814</td>\n",
       "      <td>-0.086527</td>\n",
       "      <td>1.040553</td>\n",
       "      <td>0.569527</td>\n",
       "      <td>0.062595</td>\n",
       "      <td>-0.777755</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.300687</td>\n",
       "      <td>0.700590</td>\n",
       "      <td>0.230538</td>\n",
       "      <td>0.467943</td>\n",
       "      <td>-0.170154</td>\n",
       "      <td>-0.039843</td>\n",
       "      <td>0.395929</td>\n",
       "      <td>-0.529277</td>\n",
       "      <td>-0.207355</td>\n",
       "      <td>02V56KMO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.165564</td>\n",
       "      <td>-0.371402</td>\n",
       "      <td>0.301935</td>\n",
       "      <td>-1.030253</td>\n",
       "      <td>0.479653</td>\n",
       "      <td>-0.524703</td>\n",
       "      <td>-0.219242</td>\n",
       "      <td>0.300049</td>\n",
       "      <td>0.401512</td>\n",
       "      <td>-0.024347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282781</td>\n",
       "      <td>-0.173455</td>\n",
       "      <td>0.058776</td>\n",
       "      <td>0.738854</td>\n",
       "      <td>-0.444026</td>\n",
       "      <td>0.446483</td>\n",
       "      <td>0.787220</td>\n",
       "      <td>1.445460</td>\n",
       "      <td>-0.831845</td>\n",
       "      <td>03BMGTOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.138187</td>\n",
       "      <td>-0.631721</td>\n",
       "      <td>0.032339</td>\n",
       "      <td>-0.486305</td>\n",
       "      <td>-0.156812</td>\n",
       "      <td>-0.479171</td>\n",
       "      <td>0.108851</td>\n",
       "      <td>-0.519083</td>\n",
       "      <td>0.283739</td>\n",
       "      <td>-0.401319</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.622994</td>\n",
       "      <td>-0.889408</td>\n",
       "      <td>0.353788</td>\n",
       "      <td>0.863759</td>\n",
       "      <td>-0.636728</td>\n",
       "      <td>-0.163361</td>\n",
       "      <td>0.373822</td>\n",
       "      <td>0.893028</td>\n",
       "      <td>0.078207</td>\n",
       "      <td>03LZVFM6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014786</td>\n",
       "      <td>-0.946612</td>\n",
       "      <td>0.087856</td>\n",
       "      <td>-0.276736</td>\n",
       "      <td>0.109606</td>\n",
       "      <td>-0.110407</td>\n",
       "      <td>0.312986</td>\n",
       "      <td>-1.168797</td>\n",
       "      <td>0.146922</td>\n",
       "      <td>-0.467349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.618755</td>\n",
       "      <td>-0.105546</td>\n",
       "      <td>0.909864</td>\n",
       "      <td>0.964325</td>\n",
       "      <td>-0.452088</td>\n",
       "      <td>-0.127764</td>\n",
       "      <td>0.203282</td>\n",
       "      <td>0.636816</td>\n",
       "      <td>0.432266</td>\n",
       "      <td>0EPULUM5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.117497</td>\n",
       "      <td>-0.052644</td>\n",
       "      <td>-0.101514</td>\n",
       "      <td>-0.095261</td>\n",
       "      <td>0.353081</td>\n",
       "      <td>0.906157</td>\n",
       "      <td>-0.471909</td>\n",
       "      <td>0.086664</td>\n",
       "      <td>0.482570</td>\n",
       "      <td>-0.319598</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.197256</td>\n",
       "      <td>-0.288603</td>\n",
       "      <td>0.221920</td>\n",
       "      <td>-0.763512</td>\n",
       "      <td>0.251746</td>\n",
       "      <td>-0.387666</td>\n",
       "      <td>-0.492219</td>\n",
       "      <td>0.318555</td>\n",
       "      <td>0.408116</td>\n",
       "      <td>0GM4C5GD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    embed_0   embed_1   embed_2   embed_3   embed_4   embed_5   embed_6  \\\n",
       "0  0.503976 -0.529186  0.269241  0.978526 -0.103814 -0.086527  1.040553   \n",
       "1  0.165564 -0.371402  0.301935 -1.030253  0.479653 -0.524703 -0.219242   \n",
       "2 -0.138187 -0.631721  0.032339 -0.486305 -0.156812 -0.479171  0.108851   \n",
       "3  0.014786 -0.946612  0.087856 -0.276736  0.109606 -0.110407  0.312986   \n",
       "4 -0.117497 -0.052644 -0.101514 -0.095261  0.353081  0.906157 -0.471909   \n",
       "\n",
       "    embed_7   embed_8   embed_9  ...  embed_41  embed_42  embed_43  embed_44  \\\n",
       "0  0.569527  0.062595 -0.777755  ... -0.300687  0.700590  0.230538  0.467943   \n",
       "1  0.300049  0.401512 -0.024347  ...  0.282781 -0.173455  0.058776  0.738854   \n",
       "2 -0.519083  0.283739 -0.401319  ... -0.622994 -0.889408  0.353788  0.863759   \n",
       "3 -1.168797  0.146922 -0.467349  ... -0.618755 -0.105546  0.909864  0.964325   \n",
       "4  0.086664  0.482570 -0.319598  ... -0.197256 -0.288603  0.221920 -0.763512   \n",
       "\n",
       "   embed_45  embed_46  embed_47  embed_48  embed_49        ID  \n",
       "0 -0.170154 -0.039843  0.395929 -0.529277 -0.207355  02V56KMO  \n",
       "1 -0.444026  0.446483  0.787220  1.445460 -0.831845  03BMGTOK  \n",
       "2 -0.636728 -0.163361  0.373822  0.893028  0.078207  03LZVFM6  \n",
       "3 -0.452088 -0.127764  0.203282  0.636816  0.432266  0EPULUM5  \n",
       "4  0.251746 -0.387666 -0.492219  0.318555  0.408116  0GM4C5GD  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_content_embed_DF = pd.DataFrame(test_content_embed)\n",
    "test_content_embed_DF.columns = ['embed' + \"_\" + str(x) for x in range(0, Feature_dimension)]\n",
    "test_content_embed_DF['ID'] = test_DF.ID\n",
    "print(\"Content Embed shape\", test_content_embed_DF.shape)\n",
    "\n",
    "test_content_embed_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((616, 55), (309, 51))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_content_embed_DF.shape, test_content_embed_DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep = labels\n",
    "indep = train_content_embed_DF.columns.difference(labels + ['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alcohol\n",
      "(492, 50) (492,) (124, 50) (124,)\n",
      "[0]\ttrain-logloss:0.49854\ttest-logloss:0.56275\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-logloss:0.38194\ttest-logloss:0.49438\n",
      "[2]\ttrain-logloss:0.30236\ttest-logloss:0.44479\n",
      "[3]\ttrain-logloss:0.24049\ttest-logloss:0.41284\n",
      "[4]\ttrain-logloss:0.19118\ttest-logloss:0.38661\n",
      "[5]\ttrain-logloss:0.16106\ttest-logloss:0.36276\n",
      "[6]\ttrain-logloss:0.13380\ttest-logloss:0.35031\n",
      "[7]\ttrain-logloss:0.11472\ttest-logloss:0.35026\n",
      "[8]\ttrain-logloss:0.09729\ttest-logloss:0.35422\n",
      "[9]\ttrain-logloss:0.08458\ttest-logloss:0.35092\n",
      "[10]\ttrain-logloss:0.07395\ttest-logloss:0.33769\n",
      "[11]\ttrain-logloss:0.06573\ttest-logloss:0.33459\n",
      "[12]\ttrain-logloss:0.05880\ttest-logloss:0.33294\n",
      "[13]\ttrain-logloss:0.05260\ttest-logloss:0.33437\n",
      "[14]\ttrain-logloss:0.04815\ttest-logloss:0.33813\n",
      "[15]\ttrain-logloss:0.04350\ttest-logloss:0.33927\n",
      "[16]\ttrain-logloss:0.04027\ttest-logloss:0.33961\n",
      "[17]\ttrain-logloss:0.03740\ttest-logloss:0.33880\n",
      "[18]\ttrain-logloss:0.03514\ttest-logloss:0.34365\n",
      "[19]\ttrain-logloss:0.03277\ttest-logloss:0.34412\n",
      "[20]\ttrain-logloss:0.03073\ttest-logloss:0.34540\n",
      "[21]\ttrain-logloss:0.02904\ttest-logloss:0.34581\n",
      "[22]\ttrain-logloss:0.02774\ttest-logloss:0.34665\n",
      "[23]\ttrain-logloss:0.02645\ttest-logloss:0.34534\n",
      "[24]\ttrain-logloss:0.02535\ttest-logloss:0.34675\n",
      "[25]\ttrain-logloss:0.02404\ttest-logloss:0.34487\n",
      "[26]\ttrain-logloss:0.02296\ttest-logloss:0.34527\n",
      "[27]\ttrain-logloss:0.02207\ttest-logloss:0.35001\n",
      "[28]\ttrain-logloss:0.02124\ttest-logloss:0.34772\n",
      "[29]\ttrain-logloss:0.02035\ttest-logloss:0.34464\n",
      "[30]\ttrain-logloss:0.01972\ttest-logloss:0.34603\n",
      "[31]\ttrain-logloss:0.01903\ttest-logloss:0.34706\n",
      "[32]\ttrain-logloss:0.01845\ttest-logloss:0.34686\n",
      "Stopping. Best iteration:\n",
      "[12]\ttrain-logloss:0.05880\ttest-logloss:0.33294\n",
      "\n",
      "[0]\ttrain-logloss:0.49953\ttest-logloss:0.49635\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-logloss:0.37541\ttest-logloss:0.36834\n",
      "[2]\ttrain-logloss:0.29065\ttest-logloss:0.29318\n",
      "[3]\ttrain-logloss:0.23515\ttest-logloss:0.23069\n",
      "[4]\ttrain-logloss:0.19409\ttest-logloss:0.19151\n",
      "[5]\ttrain-logloss:0.16000\ttest-logloss:0.15645\n",
      "[6]\ttrain-logloss:0.13306\ttest-logloss:0.13321\n",
      "[7]\ttrain-logloss:0.11264\ttest-logloss:0.11382\n",
      "[8]\ttrain-logloss:0.09630\ttest-logloss:0.09947\n",
      "[9]\ttrain-logloss:0.08254\ttest-logloss:0.08757\n",
      "[10]\ttrain-logloss:0.07237\ttest-logloss:0.07694\n",
      "[11]\ttrain-logloss:0.06268\ttest-logloss:0.06748\n",
      "Depression\n",
      "(492, 50) (492,) (124, 50) (124,)\n",
      "[0]\ttrain-logloss:0.50781\ttest-logloss:0.60443\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-logloss:0.39660\ttest-logloss:0.55055\n",
      "[2]\ttrain-logloss:0.31213\ttest-logloss:0.48234\n",
      "[3]\ttrain-logloss:0.25155\ttest-logloss:0.43850\n",
      "[4]\ttrain-logloss:0.20573\ttest-logloss:0.41561\n",
      "[5]\ttrain-logloss:0.17219\ttest-logloss:0.39537\n",
      "[6]\ttrain-logloss:0.14524\ttest-logloss:0.38687\n",
      "[7]\ttrain-logloss:0.12347\ttest-logloss:0.39097\n",
      "[8]\ttrain-logloss:0.10849\ttest-logloss:0.38632\n",
      "[9]\ttrain-logloss:0.09455\ttest-logloss:0.38097\n",
      "[10]\ttrain-logloss:0.08253\ttest-logloss:0.37161\n",
      "[11]\ttrain-logloss:0.07379\ttest-logloss:0.35934\n",
      "[12]\ttrain-logloss:0.06631\ttest-logloss:0.36193\n",
      "[13]\ttrain-logloss:0.06003\ttest-logloss:0.36428\n",
      "[14]\ttrain-logloss:0.05441\ttest-logloss:0.37474\n",
      "[15]\ttrain-logloss:0.04960\ttest-logloss:0.37728\n",
      "[16]\ttrain-logloss:0.04577\ttest-logloss:0.37453\n",
      "[17]\ttrain-logloss:0.04252\ttest-logloss:0.38063\n",
      "[18]\ttrain-logloss:0.03942\ttest-logloss:0.37896\n",
      "[19]\ttrain-logloss:0.03711\ttest-logloss:0.38479\n",
      "[20]\ttrain-logloss:0.03466\ttest-logloss:0.38213\n",
      "[21]\ttrain-logloss:0.03266\ttest-logloss:0.38163\n",
      "[22]\ttrain-logloss:0.03101\ttest-logloss:0.38320\n",
      "[23]\ttrain-logloss:0.02954\ttest-logloss:0.37850\n",
      "[24]\ttrain-logloss:0.02820\ttest-logloss:0.37755\n",
      "[25]\ttrain-logloss:0.02696\ttest-logloss:0.37512\n",
      "[26]\ttrain-logloss:0.02576\ttest-logloss:0.37935\n",
      "[27]\ttrain-logloss:0.02457\ttest-logloss:0.38008\n",
      "[28]\ttrain-logloss:0.02371\ttest-logloss:0.38533\n",
      "[29]\ttrain-logloss:0.02299\ttest-logloss:0.39036\n",
      "[30]\ttrain-logloss:0.02226\ttest-logloss:0.39273\n",
      "[31]\ttrain-logloss:0.02142\ttest-logloss:0.38946\n",
      "Stopping. Best iteration:\n",
      "[11]\ttrain-logloss:0.07379\ttest-logloss:0.35934\n",
      "\n",
      "[0]\ttrain-logloss:0.51417\ttest-logloss:0.51703\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-logloss:0.39651\ttest-logloss:0.38932\n",
      "[2]\ttrain-logloss:0.31194\ttest-logloss:0.30451\n",
      "[3]\ttrain-logloss:0.24820\ttest-logloss:0.24169\n",
      "[4]\ttrain-logloss:0.20469\ttest-logloss:0.19871\n",
      "[5]\ttrain-logloss:0.17064\ttest-logloss:0.16737\n",
      "[6]\ttrain-logloss:0.14459\ttest-logloss:0.13847\n",
      "[7]\ttrain-logloss:0.12552\ttest-logloss:0.11820\n",
      "[8]\ttrain-logloss:0.10829\ttest-logloss:0.10274\n",
      "[9]\ttrain-logloss:0.09410\ttest-logloss:0.08796\n",
      "[10]\ttrain-logloss:0.08358\ttest-logloss:0.07704\n",
      "Drugs\n",
      "(492, 50) (492,) (124, 50) (124,)\n",
      "[0]\ttrain-logloss:0.48384\ttest-logloss:0.51047\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-logloss:0.36712\ttest-logloss:0.42122\n",
      "[2]\ttrain-logloss:0.28446\ttest-logloss:0.34383\n",
      "[3]\ttrain-logloss:0.22358\ttest-logloss:0.30504\n",
      "[4]\ttrain-logloss:0.18143\ttest-logloss:0.27389\n",
      "[5]\ttrain-logloss:0.14832\ttest-logloss:0.24960\n",
      "[6]\ttrain-logloss:0.11992\ttest-logloss:0.24009\n",
      "[7]\ttrain-logloss:0.10019\ttest-logloss:0.23246\n",
      "[8]\ttrain-logloss:0.08360\ttest-logloss:0.23227\n",
      "[9]\ttrain-logloss:0.07125\ttest-logloss:0.22501\n",
      "[10]\ttrain-logloss:0.06252\ttest-logloss:0.20992\n",
      "[11]\ttrain-logloss:0.05506\ttest-logloss:0.21069\n",
      "[12]\ttrain-logloss:0.04859\ttest-logloss:0.20846\n",
      "[13]\ttrain-logloss:0.04369\ttest-logloss:0.20239\n",
      "[14]\ttrain-logloss:0.03975\ttest-logloss:0.19722\n",
      "[15]\ttrain-logloss:0.03554\ttest-logloss:0.19759\n",
      "[16]\ttrain-logloss:0.03259\ttest-logloss:0.19486\n",
      "[17]\ttrain-logloss:0.02996\ttest-logloss:0.19435\n",
      "[18]\ttrain-logloss:0.02761\ttest-logloss:0.19017\n",
      "[19]\ttrain-logloss:0.02566\ttest-logloss:0.19070\n",
      "[20]\ttrain-logloss:0.02400\ttest-logloss:0.19143\n",
      "[21]\ttrain-logloss:0.02226\ttest-logloss:0.18913\n",
      "[22]\ttrain-logloss:0.02082\ttest-logloss:0.18713\n",
      "[23]\ttrain-logloss:0.01969\ttest-logloss:0.18827\n",
      "[24]\ttrain-logloss:0.01864\ttest-logloss:0.18778\n",
      "[25]\ttrain-logloss:0.01769\ttest-logloss:0.18780\n",
      "[26]\ttrain-logloss:0.01697\ttest-logloss:0.18629\n",
      "[27]\ttrain-logloss:0.01627\ttest-logloss:0.18891\n",
      "[28]\ttrain-logloss:0.01582\ttest-logloss:0.18640\n",
      "[29]\ttrain-logloss:0.01529\ttest-logloss:0.18873\n",
      "[30]\ttrain-logloss:0.01488\ttest-logloss:0.18872\n",
      "[31]\ttrain-logloss:0.01435\ttest-logloss:0.18849\n",
      "[32]\ttrain-logloss:0.01391\ttest-logloss:0.18757\n",
      "[33]\ttrain-logloss:0.01361\ttest-logloss:0.18830\n",
      "[34]\ttrain-logloss:0.01320\ttest-logloss:0.18657\n",
      "[35]\ttrain-logloss:0.01273\ttest-logloss:0.18736\n",
      "[36]\ttrain-logloss:0.01237\ttest-logloss:0.19017\n",
      "[37]\ttrain-logloss:0.01213\ttest-logloss:0.19195\n",
      "[38]\ttrain-logloss:0.01185\ttest-logloss:0.19026\n",
      "[39]\ttrain-logloss:0.01153\ttest-logloss:0.18870\n",
      "[40]\ttrain-logloss:0.01128\ttest-logloss:0.19110\n",
      "[41]\ttrain-logloss:0.01103\ttest-logloss:0.19350\n",
      "[42]\ttrain-logloss:0.01080\ttest-logloss:0.19263\n",
      "[43]\ttrain-logloss:0.01065\ttest-logloss:0.19413\n",
      "[44]\ttrain-logloss:0.01040\ttest-logloss:0.19390\n",
      "[45]\ttrain-logloss:0.01028\ttest-logloss:0.19602\n",
      "[46]\ttrain-logloss:0.01012\ttest-logloss:0.19595\n",
      "Stopping. Best iteration:\n",
      "[26]\ttrain-logloss:0.01697\ttest-logloss:0.18629\n",
      "\n",
      "[0]\ttrain-logloss:0.47577\ttest-logloss:0.46565\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-logloss:0.35287\ttest-logloss:0.33730\n",
      "[2]\ttrain-logloss:0.27258\ttest-logloss:0.25516\n",
      "[3]\ttrain-logloss:0.21743\ttest-logloss:0.19754\n",
      "[4]\ttrain-logloss:0.17391\ttest-logloss:0.15540\n",
      "[5]\ttrain-logloss:0.14264\ttest-logloss:0.12415\n",
      "[6]\ttrain-logloss:0.11975\ttest-logloss:0.10060\n",
      "[7]\ttrain-logloss:0.09985\ttest-logloss:0.08223\n",
      "[8]\ttrain-logloss:0.08358\ttest-logloss:0.06928\n",
      "[9]\ttrain-logloss:0.07124\ttest-logloss:0.05725\n",
      "[10]\ttrain-logloss:0.06157\ttest-logloss:0.04791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11]\ttrain-logloss:0.05382\ttest-logloss:0.04182\n",
      "[12]\ttrain-logloss:0.04751\ttest-logloss:0.03629\n",
      "[13]\ttrain-logloss:0.04248\ttest-logloss:0.03299\n",
      "[14]\ttrain-logloss:0.03825\ttest-logloss:0.02917\n",
      "[15]\ttrain-logloss:0.03423\ttest-logloss:0.02666\n",
      "[16]\ttrain-logloss:0.03106\ttest-logloss:0.02432\n",
      "[17]\ttrain-logloss:0.02828\ttest-logloss:0.02253\n",
      "[18]\ttrain-logloss:0.02588\ttest-logloss:0.02099\n",
      "[19]\ttrain-logloss:0.02400\ttest-logloss:0.01911\n",
      "[20]\ttrain-logloss:0.02239\ttest-logloss:0.01834\n",
      "[21]\ttrain-logloss:0.02096\ttest-logloss:0.01708\n",
      "[22]\ttrain-logloss:0.01972\ttest-logloss:0.01610\n",
      "[23]\ttrain-logloss:0.01868\ttest-logloss:0.01524\n",
      "[24]\ttrain-logloss:0.01775\ttest-logloss:0.01456\n",
      "[25]\ttrain-logloss:0.01688\ttest-logloss:0.01359\n",
      "Suicide\n",
      "(492, 50) (492,) (124, 50) (124,)\n",
      "[0]\ttrain-logloss:0.50949\ttest-logloss:0.53041\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-logloss:0.40097\ttest-logloss:0.43965\n",
      "[2]\ttrain-logloss:0.31700\ttest-logloss:0.39403\n",
      "[3]\ttrain-logloss:0.25009\ttest-logloss:0.34622\n",
      "[4]\ttrain-logloss:0.19548\ttest-logloss:0.32616\n",
      "[5]\ttrain-logloss:0.16147\ttest-logloss:0.31069\n",
      "[6]\ttrain-logloss:0.13226\ttest-logloss:0.30332\n",
      "[7]\ttrain-logloss:0.11097\ttest-logloss:0.30275\n",
      "[8]\ttrain-logloss:0.09328\ttest-logloss:0.29199\n",
      "[9]\ttrain-logloss:0.07987\ttest-logloss:0.28764\n",
      "[10]\ttrain-logloss:0.07019\ttest-logloss:0.28398\n",
      "[11]\ttrain-logloss:0.06150\ttest-logloss:0.28253\n",
      "[12]\ttrain-logloss:0.05488\ttest-logloss:0.28843\n",
      "[13]\ttrain-logloss:0.04875\ttest-logloss:0.29721\n",
      "[14]\ttrain-logloss:0.04435\ttest-logloss:0.29388\n",
      "[15]\ttrain-logloss:0.04033\ttest-logloss:0.29534\n",
      "[16]\ttrain-logloss:0.03687\ttest-logloss:0.29766\n",
      "[17]\ttrain-logloss:0.03441\ttest-logloss:0.30323\n",
      "[18]\ttrain-logloss:0.03159\ttest-logloss:0.30654\n",
      "[19]\ttrain-logloss:0.02958\ttest-logloss:0.30935\n",
      "[20]\ttrain-logloss:0.02789\ttest-logloss:0.31229\n",
      "[21]\ttrain-logloss:0.02637\ttest-logloss:0.31690\n",
      "[22]\ttrain-logloss:0.02500\ttest-logloss:0.32155\n",
      "[23]\ttrain-logloss:0.02366\ttest-logloss:0.32559\n",
      "[24]\ttrain-logloss:0.02268\ttest-logloss:0.32614\n",
      "[25]\ttrain-logloss:0.02153\ttest-logloss:0.32640\n",
      "[26]\ttrain-logloss:0.02052\ttest-logloss:0.33293\n",
      "[27]\ttrain-logloss:0.01978\ttest-logloss:0.33680\n",
      "[28]\ttrain-logloss:0.01909\ttest-logloss:0.34063\n",
      "[29]\ttrain-logloss:0.01835\ttest-logloss:0.34005\n",
      "[30]\ttrain-logloss:0.01775\ttest-logloss:0.34266\n",
      "[31]\ttrain-logloss:0.01709\ttest-logloss:0.34451\n",
      "Stopping. Best iteration:\n",
      "[11]\ttrain-logloss:0.06150\ttest-logloss:0.28253\n",
      "\n",
      "[0]\ttrain-logloss:0.49411\ttest-logloss:0.49286\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 20 rounds.\n",
      "[1]\ttrain-logloss:0.38465\ttest-logloss:0.38259\n",
      "[2]\ttrain-logloss:0.30947\ttest-logloss:0.30795\n",
      "[3]\ttrain-logloss:0.25565\ttest-logloss:0.25227\n",
      "[4]\ttrain-logloss:0.20702\ttest-logloss:0.20835\n",
      "[5]\ttrain-logloss:0.17024\ttest-logloss:0.17582\n",
      "[6]\ttrain-logloss:0.14292\ttest-logloss:0.14413\n",
      "[7]\ttrain-logloss:0.11836\ttest-logloss:0.12004\n",
      "[8]\ttrain-logloss:0.10088\ttest-logloss:0.10333\n",
      "[9]\ttrain-logloss:0.08676\ttest-logloss:0.08924\n",
      "[10]\ttrain-logloss:0.07528\ttest-logloss:0.07862\n",
      "{'Alcohol': 0.332936, 'Depression': 0.359338, 'Drugs': 0.186286, 'Suicide': 0.282525}\n"
     ]
    }
   ],
   "source": [
    "num_rounds = 10000\n",
    "params = {'objective' : 'binary:logistic'\n",
    "          ,'eval_metric': 'logloss'\n",
    "          ,'max_depth' : 6\n",
    "          ,'eta' : 0.3\n",
    "          ,'subsample': 1\n",
    "          ,'colsample_bytree': 1\n",
    "          #,'tree_method' : 'gpu_hist'\n",
    "          }\n",
    "\n",
    "final_output = pd.DataFrame()\n",
    "final_output['ID'] = test_DF['ID']\n",
    "score_dict = {}\n",
    "for lab in labels:\n",
    "    print(lab)\n",
    "    np.random.seed(100)\n",
    "    train_local_X, valid_local_X, train_local_Y, valid_local_Y = train_test_split(train_content_embed_DF[indep],\n",
    "                                                                                  train_content_embed_DF[lab],\n",
    "                                                                                  test_size = 0.2,\n",
    "                                                                                  random_state = 100,\n",
    "                                                                                  stratify = train_content_embed_DF[lab])\n",
    "    \n",
    "    print(train_local_X.shape, train_local_Y.shape, valid_local_X.shape, valid_local_Y.shape)\n",
    "    \n",
    "    dtrain_local = xgb.DMatrix(data = train_local_X, label = train_local_Y)\n",
    "    dtest_local  = xgb.DMatrix(data = valid_local_X, label = valid_local_Y)\n",
    "    dtrain_prod = xgb.DMatrix(data = train_content_embed_DF[indep], label = train_content_embed_DF[lab])\n",
    "    dtest_prod = xgb.DMatrix(data = test_content_embed_DF[indep])\n",
    "\n",
    "    eval_set = [(dtrain_local,'train'), (dtest_local,'test')]\n",
    "\n",
    "    np.random.seed(100)\n",
    "    local_model = xgb.train(params,\n",
    "                            dtrain_local,\n",
    "                            evals = eval_set,\n",
    "                            num_boost_round = num_rounds,\n",
    "                            verbose_eval = True,\n",
    "                            early_stopping_rounds = 20\n",
    "                                       )\n",
    "    best_iteration = local_model.best_iteration\n",
    "    best_score = local_model.best_score\n",
    "\n",
    "    score_dict[lab] = best_score\n",
    "    \n",
    "    np.random.seed(100)\n",
    "    clf = xgb.train(params,\n",
    "                    dtrain_prod,\n",
    "                    num_boost_round = best_iteration,\n",
    "                    evals = eval_set,\n",
    "                    verbose_eval = True,\n",
    "                    early_stopping_rounds = 20\n",
    "                               )\n",
    "\n",
    "    xgb_prod_predict = clf.predict(dtest_prod)\n",
    "\n",
    "    final_output[lab] = xgb_prod_predict\n",
    "\n",
    "final_output = final_output[['ID','Depression', 'Alcohol', 'Suicide', 'Drugs']]\n",
    "\n",
    "print(score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = final_output[['ID', 'Depression', 'Alcohol', 'Suicide', 'Drugs']]\n",
    "final_output.to_csv('../output/sub_25_XGBoost.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
