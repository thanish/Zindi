{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "import re \n",
    "\n",
    "from random import seed\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf    \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, GRU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "\n",
    "import sympound \n",
    "from autocorrect import Speller\n",
    "import splitter\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec as w2v\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\thanisb\\\\Documents\\\\Competition\\\\Zindi\\\\Tech4MentalHealth\\\\Notebook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_DF = pd.read_csv('../data/train_corrected.csv')\n",
    "test_DF = pd.read_csv('../data/test_corrected.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Drugs</th>\n",
       "      <th>Suicide</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUAVK39Z</td>\n",
       "      <td>i feel that it was better i dream happy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9JDAGUV3</td>\n",
       "      <td>why do i get hallucinations</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>419WR1LQ</td>\n",
       "      <td>i am stressed due to lack of financial support...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6UY7DX6Q</td>\n",
       "      <td>why is life important</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FYC0FTFB</td>\n",
       "      <td>how could i be helped to go through the depres...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>BOHSNXCN</td>\n",
       "      <td>what should i do to stop alcoholism</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>GVDXRQPY</td>\n",
       "      <td>how to become my oneself again</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>IO4JHIQS</td>\n",
       "      <td>how can someone stop it</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>1DS3P1XO</td>\n",
       "      <td>i feel unworthy</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>ORF71PVQ</td>\n",
       "      <td>i feel so discouraged with life</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>616 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID                                               text  Alcohol  \\\n",
       "0    SUAVK39Z            i feel that it was better i dream happy        0   \n",
       "1    9JDAGUV3                        why do i get hallucinations        0   \n",
       "2    419WR1LQ  i am stressed due to lack of financial support...        0   \n",
       "3    6UY7DX6Q                              why is life important        0   \n",
       "4    FYC0FTFB  how could i be helped to go through the depres...        0   \n",
       "..        ...                                                ...      ...   \n",
       "611  BOHSNXCN                what should i do to stop alcoholism        1   \n",
       "612  GVDXRQPY                     how to become my oneself again        0   \n",
       "613  IO4JHIQS                            how can someone stop it        1   \n",
       "614  1DS3P1XO                                    i feel unworthy        0   \n",
       "615  ORF71PVQ                    i feel so discouraged with life        0   \n",
       "\n",
       "     Depression  Drugs  Suicide  \n",
       "0             1      0        0  \n",
       "1             0      1        0  \n",
       "2             1      0        0  \n",
       "3             0      0        1  \n",
       "4             1      0        0  \n",
       "..          ...    ...      ...  \n",
       "611           0      0        0  \n",
       "612           0      0        1  \n",
       "613           0      0        0  \n",
       "614           1      0        0  \n",
       "615           1      0        0  \n",
       "\n",
       "[616 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the label to OHE\n",
    "train_DF = pd.concat([train_DF[['ID', 'text']], pd.get_dummies(train_DF.label)], axis = 1)\n",
    "labels = ['Alcohol', 'Depression', 'Drugs', 'Suicide']\n",
    "train_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, remove_stopwords = True):\n",
    "    try:\n",
    "        #print(text)\n",
    "        processed_text = text.lower()\n",
    "        processed_text = re.sub(\"[^a-zA-Z]\",\" \",processed_text)\n",
    "        processed_text = processed_text.lower().split()\n",
    "        #print(\"processed\", processed_text)\n",
    "        words = processed_text\n",
    "        if remove_stopwords:\n",
    "                stops = set(stopwords.words(\"english\"))     \n",
    "                words = [w for w in processed_text if not w in stops]\n",
    "    except AttributeError:  # handling the case where the token is empty\n",
    "        words = ''\n",
    "    \n",
    "    return words\n",
    "\n",
    "def review_sentences(review, tokenizer, remove_stopwords=True):\n",
    "    # 1. Using nltk tokenizer\n",
    "    try:\n",
    "        raw_sentences = tokenizer.tokenize(review.strip())\n",
    "        \n",
    "    except AttributeError:  # handling the case where the token is empty\n",
    "        raw_sentences = ''\n",
    "\n",
    "    if len(raw_sentences) > 1 : raw_sentences = [\" \".join(raw_sentences)]\n",
    "\n",
    "    sentences = []\n",
    "    # 2. Loop for each sentence\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)>0:\n",
    "            sentences.append(preprocessing(raw_sentence, remove_stopwords))\n",
    "\n",
    "    # This returns the list of lists\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a Word2Vec model\n",
    "def w2vec_model(text, feature_embed):\n",
    "    num_features = feature_embed  # Word vector dimensionality\n",
    "    min_word_count = 1  # Minimum word count\n",
    "    num_workers = 4     # Number of parallel threads\n",
    "    context = 10        # Context window size\n",
    "    downsampling = 1e-3 # (0.001) Downsample setting for frequent words\n",
    "\n",
    "    print(\"Training model....\")\n",
    "    model = w2v.Word2Vec(text,\n",
    "                         workers=num_workers,\n",
    "                         size=num_features,\n",
    "                         min_count=min_word_count,\n",
    "                         window=context,\n",
    "                         sample=downsampling\n",
    "    )\n",
    "\n",
    "    #model.build_vocab(sentence)\n",
    "    model.train(text, total_examples= model.corpus_count, epochs=300)\n",
    "\n",
    "    # # To make the model memory efficient\n",
    "    # model.init_sims(replace=True)\n",
    "\n",
    "    # # Saving the model for later use. Can be loaded using Word2Vec.load()\n",
    "    # model_name = \"300features_40minwords_10context\"\n",
    "    # model.save(model_name)\n",
    "\n",
    "    print(\"Vocabulary shape\", model.wv.syn0.shape)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureVecMethod_v2(words, model, features_dim):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.empty(shape=[0, features_dim],  dtype=\"float32\")\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            word_vector = model[word]\n",
    "            featureVec = np.concatenate((featureVec, word_vector.reshape(-1, features_dim)), axis=0)        \n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs_v2(reviews, model, sent_len, features_dim):\n",
    "    reviewFeatureVecs = np.zeros((len(reviews), sent_len, features_dim),dtype=\"float32\")\n",
    "    for i, review in enumerate(reviews):\n",
    "        vectors = featureVecMethod_v2(review, model, features_dim)\n",
    "        reviewFeatureVecs[i] = vectors\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616\n",
      "309\n",
      "925\n",
      "Training model....\n",
      "Vocabulary shape (1000, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thanisb\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n"
     ]
    }
   ],
   "source": [
    "Feature_dimension = 300\n",
    "Max_Len = 196\n",
    "\n",
    "train_content_sentence = []\n",
    "# TOkenizing all the text to\n",
    "for i, sent in enumerate(train_DF.text):\n",
    "    content = review_sentences(sent, tokenizer, remove_stopwords=False)\n",
    "    pad_len = Max_Len - len(content[0])\n",
    "    content = list(content[0] + (' PAD' * pad_len).split())\n",
    "    train_content_sentence.append(content)\n",
    "    #train_content_sentence += content\n",
    "print(len(train_content_sentence))\n",
    "\n",
    "test_content_sentence = []\n",
    "for i, sent in enumerate(test_DF.text):\n",
    "    content = review_sentences(sent, tokenizer, remove_stopwords=False)\n",
    "    pad_len = Max_Len - len(content[0])\n",
    "    content = list(content[0] + (' PAD' * pad_len).split())\n",
    "    test_content_sentence.append(content)\n",
    "    #test_content_sentence += content\n",
    "print(len(test_content_sentence))\n",
    "    \n",
    "\n",
    "Overall_content_sentence = train_content_sentence + test_content_sentence\n",
    "print(len(Overall_content_sentence))\n",
    "\n",
    "model = w2vec_model(Overall_content_sentence, feature_embed = Feature_dimension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thanisb\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(616, 196, 300)\n",
      "(309, 196, 300)\n"
     ]
    }
   ],
   "source": [
    "train_content_embed = getAvgFeatureVecs_v2(train_content_sentence, \n",
    "                                           model, \n",
    "                                           sent_len = Max_Len, \n",
    "                                           features_dim = Feature_dimension)\n",
    "print(train_content_embed.shape)\n",
    "\n",
    "test_content_embed = getAvgFeatureVecs_v2(test_content_sentence, \n",
    "                                          model, \n",
    "                                          sent_len = Max_Len,\n",
    "                                          features_dim = Feature_dimension)\n",
    "print(test_content_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(492, 196, 300) (124, 196, 300) (492, 4) (124, 4)\n"
     ]
    }
   ],
   "source": [
    "dep = labels\n",
    "indep = train_DF.columns.difference(labels + ['ID'])\n",
    "\n",
    "np.random.seed(100)\n",
    "train_local_X, valid_local_X, train_local_Y, valid_local_Y = train_test_split(train_content_embed,\n",
    "                                                                              train_DF[dep].values,\n",
    "                                                                              test_size = 0.2,\n",
    "                                                                              random_state = 100)\n",
    "print(train_local_X.shape, valid_local_X.shape, train_local_Y.shape, valid_local_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "tf.random.set_seed(100)\n",
    "np.random.seed(100)\n",
    "seed(100)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(100, return_sequences = True, input_shape = (train_local_X.shape[1], train_local_X.shape[2]), activation = 'relu'))\n",
    "model.add(GRU(50, return_sequences = False, activation = 'relu'))\n",
    "# model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(len(dep), activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss= 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy', 'binary_crossentropy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 492 samples, validate on 124 samples\n",
      "Epoch 1/100\n",
      "492/492 [==============================] - 4s 9ms/step - loss: 0.4930 - accuracy: 0.7693 - binary_crossentropy: 0.4930 - val_loss: 0.4805 - val_accuracy: 0.7823 - val_binary_crossentropy: 0.4805\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48053, saving model to ../output/best_local_model.hdf5\n",
      "Epoch 2/100\n",
      "492/492 [==============================] - 5s 10ms/step - loss: 0.4684 - accuracy: 0.7907 - binary_crossentropy: 0.4684 - val_loss: 0.4660 - val_accuracy: 0.8085 - val_binary_crossentropy: 0.4660\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.48053 to 0.46601, saving model to ../output/best_local_model.hdf5\n",
      "Epoch 3/100\n",
      "492/492 [==============================] - 5s 10ms/step - loss: 0.4396 - accuracy: 0.8171 - binary_crossentropy: 0.4396 - val_loss: 0.4201 - val_accuracy: 0.8528 - val_binary_crossentropy: 0.4201\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.46601 to 0.42013, saving model to ../output/best_local_model.hdf5\n",
      "Epoch 4/100\n",
      "492/492 [==============================] - 5s 9ms/step - loss: 0.3911 - accuracy: 0.8547 - binary_crossentropy: 0.3911 - val_loss: 0.3825 - val_accuracy: 0.8589 - val_binary_crossentropy: 0.3825\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.42013 to 0.38254, saving model to ../output/best_local_model.hdf5\n",
      "Epoch 5/100\n",
      "492/492 [==============================] - 4s 9ms/step - loss: 0.3379 - accuracy: 0.8791 - binary_crossentropy: 0.3379 - val_loss: 0.3470 - val_accuracy: 0.8750 - val_binary_crossentropy: 0.3470\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.38254 to 0.34695, saving model to ../output/best_local_model.hdf5\n",
      "Epoch 6/100\n",
      "492/492 [==============================] - 4s 9ms/step - loss: 0.2889 - accuracy: 0.8958 - binary_crossentropy: 0.2889 - val_loss: 0.3125 - val_accuracy: 0.8871 - val_binary_crossentropy: 0.3125\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.34695 to 0.31252, saving model to ../output/best_local_model.hdf5\n",
      "Epoch 7/100\n",
      "492/492 [==============================] - 4s 9ms/step - loss: 0.2491 - accuracy: 0.8994 - binary_crossentropy: 0.2491 - val_loss: 0.2945 - val_accuracy: 0.8810 - val_binary_crossentropy: 0.2945\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.31252 to 0.29452, saving model to ../output/best_local_model.hdf5\n",
      "Epoch 8/100\n",
      "492/492 [==============================] - 4s 9ms/step - loss: 0.2206 - accuracy: 0.9040 - binary_crossentropy: 0.2206 - val_loss: 0.2766 - val_accuracy: 0.8851 - val_binary_crossentropy: 0.2766\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.29452 to 0.27663, saving model to ../output/best_local_model.hdf5\n",
      "Epoch 9/100\n",
      "492/492 [==============================] - 5s 10ms/step - loss: 0.1918 - accuracy: 0.9141 - binary_crossentropy: 0.1918 - val_loss: 0.2925 - val_accuracy: 0.8831 - val_binary_crossentropy: 0.2925\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.27663\n",
      "Epoch 10/100\n",
      "492/492 [==============================] - 5s 9ms/step - loss: 0.1733 - accuracy: 0.9217 - binary_crossentropy: 0.1733 - val_loss: 0.3333 - val_accuracy: 0.8851 - val_binary_crossentropy: 0.3333\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.27663\n",
      "Epoch 11/100\n",
      "492/492 [==============================] - 4s 9ms/step - loss: 0.1623 - accuracy: 0.9243 - binary_crossentropy: 0.1623 - val_loss: 0.4332 - val_accuracy: 0.8750 - val_binary_crossentropy: 0.4332\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.27663\n",
      "Epoch 12/100\n",
      "492/492 [==============================] - 5s 9ms/step - loss: 0.1592 - accuracy: 0.9253 - binary_crossentropy: 0.1592 - val_loss: 0.3565 - val_accuracy: 0.8831 - val_binary_crossentropy: 0.3565\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.27663\n",
      "Epoch 13/100\n",
      "492/492 [==============================] - 4s 9ms/step - loss: 0.1351 - accuracy: 0.9334 - binary_crossentropy: 0.1351 - val_loss: 0.3222 - val_accuracy: 0.8931 - val_binary_crossentropy: 0.3222\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.27663\n",
      "Epoch 14/100\n",
      "492/492 [==============================] - 4s 9ms/step - loss: 0.1223 - accuracy: 0.9502 - binary_crossentropy: 0.1223 - val_loss: 0.4136 - val_accuracy: 0.8831 - val_binary_crossentropy: 0.4136\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.27663\n",
      "Epoch 15/100\n",
      "492/492 [==============================] - 5s 9ms/step - loss: 0.1053 - accuracy: 0.9568 - binary_crossentropy: 0.1053 - val_loss: 0.3934 - val_accuracy: 0.8790 - val_binary_crossentropy: 0.3934\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.27663\n",
      "Epoch 16/100\n",
      "492/492 [==============================] - 5s 10ms/step - loss: 0.0924 - accuracy: 0.9649 - binary_crossentropy: 0.0924 - val_loss: 0.3756 - val_accuracy: 0.8871 - val_binary_crossentropy: 0.3756\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.27663\n",
      "Epoch 17/100\n",
      "492/492 [==============================] - 4s 9ms/step - loss: 0.0798 - accuracy: 0.9746 - binary_crossentropy: 0.0798 - val_loss: 0.4661 - val_accuracy: 0.8750 - val_binary_crossentropy: 0.4661\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.27663\n",
      "Epoch 18/100\n",
      "492/492 [==============================] - 4s 9ms/step - loss: 0.0761 - accuracy: 0.9766 - binary_crossentropy: 0.0761 - val_loss: 0.3941 - val_accuracy: 0.8810 - val_binary_crossentropy: 0.3941\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.27663\n",
      "Epoch 19/100\n",
      "492/492 [==============================] - 5s 9ms/step - loss: 0.1339 - accuracy: 0.9502 - binary_crossentropy: 0.1339 - val_loss: 0.4287 - val_accuracy: 0.8891 - val_binary_crossentropy: 0.4287\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.27663\n",
      "Epoch 20/100\n",
      "492/492 [==============================] - 4s 9ms/step - loss: 0.1043 - accuracy: 0.9624 - binary_crossentropy: 0.1043 - val_loss: 0.3597 - val_accuracy: 0.8710 - val_binary_crossentropy: 0.3597\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.27663\n",
      "Epoch 21/100\n",
      "492/492 [==============================] - 5s 9ms/step - loss: 0.0728 - accuracy: 0.9766 - binary_crossentropy: 0.0728 - val_loss: 0.3513 - val_accuracy: 0.8911 - val_binary_crossentropy: 0.3513\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.27663\n",
      "Epoch 22/100\n",
      "492/492 [==============================] - 5s 10ms/step - loss: 0.0609 - accuracy: 0.9782 - binary_crossentropy: 0.0609 - val_loss: 0.4021 - val_accuracy: 0.8810 - val_binary_crossentropy: 0.4021\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.27663\n",
      "Epoch 23/100\n",
      "492/492 [==============================] - 5s 9ms/step - loss: 0.0395 - accuracy: 0.9903 - binary_crossentropy: 0.0395 - val_loss: 0.4305 - val_accuracy: 0.8810 - val_binary_crossentropy: 0.4305\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.27663\n",
      "Epoch 00023: early stopping\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss'\n",
    "                           ,verbose = True\n",
    "                           ,mode = 'min'\n",
    "                           ,patience = 15\n",
    "                           #,min_delta=0.0001\n",
    "                          )\n",
    "checkpoint = ModelCheckpoint(monitor = 'val_loss',\n",
    "                             mode = 'min',\n",
    "                             filepath = '../output/best_local_model.hdf5', \n",
    "                             verbose = 1, \n",
    "                             save_best_only = True)\n",
    "\n",
    "history = model.fit(train_local_X, train_local_Y, \n",
    "                    epochs=epochs, \n",
    "                    batch_size= batch_size,\n",
    "                    #validation_split= 0.2,\n",
    "                    validation_data= (valid_local_X, valid_local_Y),\n",
    "                    callbacks=[early_stop, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 0, 2, 1, 0, 1, 1, 3, 1, 3, 0, 0, 1, 0, 2, 0, 0, 1, 3, 1, 1,\n",
       "        1, 0, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 3,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 0, 3, 0, 1, 1, 2, 1, 1, 1, 0, 0, 1, 1, 2,\n",
       "        1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 3, 0, 1, 3, 3, 3, 1, 3, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 3, 1, 1, 1, 1, 2, 1, 3, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1], dtype=int64),\n",
       " array([2, 1, 1, 1, 0, 1, 1, 1, 0, 2, 1, 0, 1, 1, 0, 1, 1, 0, 1, 3, 1, 1,\n",
       "        2, 1, 3, 1, 1, 1, 1, 0, 1, 1, 3, 1, 1, 0, 1, 3, 1, 0, 1, 1, 0, 1,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 3, 1, 0, 0, 0, 1, 1, 1, 2,\n",
       "        3, 1, 3, 1, 1, 3, 2, 0, 1, 1, 0, 3, 1, 3, 3, 1, 3, 0, 1, 1, 0, 0,\n",
       "        1, 3, 1, 3, 1, 1, 0, 1, 1, 1, 1, 1, 1, 3, 3, 3, 1, 3, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 3, 0, 3, 0, 1, 0, 3, 1, 1, 3, 0, 1, 3, 0, 1, 1, 0, 0,\n",
       "        1, 3, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 3, 2, 1, 0, 1, 1,\n",
       "        0, 3, 1, 0, 1, 1, 0, 3, 0, 2, 0, 3, 1, 1, 1, 1, 1, 0, 2, 0, 0, 0,\n",
       "        1, 1, 0, 2, 1, 3, 1, 0, 0, 0, 1, 0, 0, 0, 3, 1, 1, 2, 1, 1, 0, 2,\n",
       "        1, 1, 3, 0, 1, 3, 1, 0, 1, 0, 1, 1, 1, 0, 3, 0, 1, 3, 0, 0, 1, 1,\n",
       "        1, 3, 3, 1, 3, 0, 2, 1, 1, 1, 1, 1, 1, 1, 3, 1, 0, 2, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 0, 2, 1, 0, 1, 3, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "        3, 0, 0, 0, 1, 3, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 2, 0, 1,\n",
       "        1, 3, 0, 1, 1, 1, 1, 1, 2, 3, 2, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "        3], dtype=int64))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running the prediction\n",
    "model.load_weights(\"../output/best_local_model.hdf5\")\n",
    "model.predict(valid_local_X).argmax(axis = 1), model.predict(test_content_embed).argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Depression</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Suicide</th>\n",
       "      <th>Drugs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>02V56KMO</td>\n",
       "      <td>0.049698</td>\n",
       "      <td>0.312891</td>\n",
       "      <td>0.322518</td>\n",
       "      <td>0.330993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03BMGTOK</td>\n",
       "      <td>0.978126</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.003244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03LZVFM6</td>\n",
       "      <td>0.960782</td>\n",
       "      <td>0.003269</td>\n",
       "      <td>0.014660</td>\n",
       "      <td>0.017179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0EPULUM5</td>\n",
       "      <td>0.394141</td>\n",
       "      <td>0.021279</td>\n",
       "      <td>0.064595</td>\n",
       "      <td>0.075042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0GM4C5GD</td>\n",
       "      <td>0.028937</td>\n",
       "      <td>0.371191</td>\n",
       "      <td>0.257797</td>\n",
       "      <td>0.358780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Z9A6ACLK</td>\n",
       "      <td>0.934757</td>\n",
       "      <td>0.011615</td>\n",
       "      <td>0.053264</td>\n",
       "      <td>0.044841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>ZDUOIGKN</td>\n",
       "      <td>0.853093</td>\n",
       "      <td>0.023830</td>\n",
       "      <td>0.089158</td>\n",
       "      <td>0.056398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>ZHQ60CCH</td>\n",
       "      <td>0.628944</td>\n",
       "      <td>0.057558</td>\n",
       "      <td>0.166942</td>\n",
       "      <td>0.091595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>ZVIJMA4O</td>\n",
       "      <td>0.009613</td>\n",
       "      <td>0.488795</td>\n",
       "      <td>0.127024</td>\n",
       "      <td>0.294661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>ZYIFAY98</td>\n",
       "      <td>0.167372</td>\n",
       "      <td>0.178794</td>\n",
       "      <td>0.300701</td>\n",
       "      <td>0.201952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>309 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID  Depression   Alcohol   Suicide     Drugs\n",
       "0    02V56KMO    0.049698  0.312891  0.322518  0.330993\n",
       "1    03BMGTOK    0.978126  0.000456  0.001767  0.003244\n",
       "2    03LZVFM6    0.960782  0.003269  0.014660  0.017179\n",
       "3    0EPULUM5    0.394141  0.021279  0.064595  0.075042\n",
       "4    0GM4C5GD    0.028937  0.371191  0.257797  0.358780\n",
       "..        ...         ...       ...       ...       ...\n",
       "304  Z9A6ACLK    0.934757  0.011615  0.053264  0.044841\n",
       "305  ZDUOIGKN    0.853093  0.023830  0.089158  0.056398\n",
       "306  ZHQ60CCH    0.628944  0.057558  0.166942  0.091595\n",
       "307  ZVIJMA4O    0.009613  0.488795  0.127024  0.294661\n",
       "308  ZYIFAY98    0.167372  0.178794  0.300701  0.201952\n",
       "\n",
       "[309 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_output = model.predict(test_content_embed)#.argmax(axis = 1)\n",
    "\n",
    "final_output = pd.DataFrame(predicted_output)\n",
    "final_output.columns = ['Alcohol', 'Depression', 'Drugs', 'Suicide']\n",
    "final_output['ID'] = test_DF.ID\n",
    "\n",
    "final_output = final_output[['ID', 'Depression', 'Alcohol', 'Suicide', 'Drugs']]\n",
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = final_output[['ID', 'Depression', 'Alcohol', 'Suicide', 'Drugs']]\n",
    "final_output.to_csv('../output/sub_42_GRU.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
